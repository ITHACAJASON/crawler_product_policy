#!/usr/bin/env python3
"""
æµ‹è¯•çˆ¬è™«è„šæœ¬ - ä¸Playwright MCPååŒå·¥ä½œ
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from src.crawlers.national_crawler import NationalCrawler
from src.models.policy_model import PolicyDocument
from bs4 import BeautifulSoup
from loguru import logger

class PlaywrightTestCrawler:
    """ä¸Playwright MCPååŒçš„æµ‹è¯•çˆ¬è™«"""
    
    def __init__(self):
        self.crawler = NationalCrawler()
        logger.info("æµ‹è¯•çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    def parse_current_page_html(self, html_content):
        """è§£æå½“å‰é¡µé¢çš„HTMLå†…å®¹"""
        documents = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            logger.info(f"HTMLå†…å®¹é•¿åº¦: {len(html_content)}")
            
            # æŸ¥æ‰¾æ”¿ç­–æ–‡æ¡£é¡¹ç›®
            # ä»é¡µé¢å¿«ç…§å¯ä»¥çœ‹åˆ°ï¼Œæ”¿ç­–é¡¹ç›®åœ¨ generic å®¹å™¨ä¸­
            policy_sections = soup.select('div.searchResult') or soup.select('.result-list')
            
            if not policy_sections:
                # å°è¯•æŸ¥æ‰¾å…·ä½“çš„æ”¿ç­–é“¾æ¥
                policy_links = soup.select('a[href*="content"]')
                logger.info(f"æ‰¾åˆ° {len(policy_links)} ä¸ªæ”¿ç­–é“¾æ¥")
                
                for i, link in enumerate(policy_links[:10]):  # é™åˆ¶å‰10ä¸ªè¿›è¡Œæµ‹è¯•
                    try:
                        doc = PolicyDocument()
                        
                        # æå–æ ‡é¢˜
                        title_text = link.get_text(strip=True)
                        if title_text and len(title_text) > 5:
                            doc.policy_title = title_text
                            doc.source_url = link.get('href', '')
                            if doc.source_url and not doc.source_url.startswith('http'):
                                doc.source_url = f"http://www.gov.cn{doc.source_url}"
                            
                            # è®¾ç½®åŸºæœ¬ä¿¡æ¯
                            doc.policy_level = "ä¸­å¤®"
                            doc.publish_agency = "ä¸­å¤®æ”¿åºœ"
                            doc.keywords = ["å…»è€"]
                            doc.doc_id = f"test_{i}"
                            
                            # ä»æ ‡é¢˜æ¨æ–­æ–‡ä»¶ç±»å‹
                            if 'é€šçŸ¥' in title_text:
                                doc.document_type = 'é€šçŸ¥'
                            elif 'æ„è§' in title_text:
                                doc.document_type = 'æ„è§'
                            elif 'åŠæ³•' in title_text:
                                doc.document_type = 'åŠæ³•'
                            elif 'è§„å®š' in title_text:
                                doc.document_type = 'è§„å®š'
                            else:
                                doc.document_type = 'æ”¿ç­–æ–‡ä»¶'
                            
                            # å°è¯•ä»çˆ¶å…ƒç´ è·å–å‘å¸ƒæ—¶é—´
                            parent = link.parent
                            date_pattern = parent.get_text() if parent else ""
                            import re
                            date_match = re.search(r'(\d{4}-\d{1,2}-\d{1,2})', date_pattern)
                            if date_match:
                                doc.publish_date = date_match.group(1)
                            
                            documents.append(doc)
                            logger.info(f"è§£æåˆ°æ”¿ç­–: {doc.policy_title[:50]}...")
                    
                    except Exception as e:
                        logger.error(f"è§£æç¬¬{i}ä¸ªæ”¿ç­–é¡¹ç›®å¤±è´¥: {str(e)}")
                        continue
            
            logger.info(f"æ€»å…±è§£æåˆ° {len(documents)} ä¸ªæ”¿ç­–æ–‡æ¡£")
            return documents
            
        except Exception as e:
            logger.error(f"è§£æHTMLå¤±è´¥: {str(e)}")
            return []
    
    def test_save_documents(self, documents):
        """æµ‹è¯•ä¿å­˜æ–‡æ¡£åŠŸèƒ½"""
        if not documents:
            logger.warning("æ²¡æœ‰æ–‡æ¡£éœ€è¦ä¿å­˜")
            return False
        
        try:
            # ä½¿ç”¨çˆ¬è™«çš„ä¿å­˜åŠŸèƒ½
            for doc in documents:
                self.crawler.add_document(doc)
            
            # æ‰‹åŠ¨è§¦å‘æœ€ç»ˆä¿å­˜
            self.crawler._final_save()
            
            logger.info(f"æˆåŠŸä¿å­˜ {len(documents)} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§ª æ”¿ç­–çˆ¬è™«æµ‹è¯•ç¨‹åº")
    print("=" * 60)
    print("æ­¤ç¨‹åºå°†é…åˆPlaywright MCPæµ‹è¯•çˆ¬è™«åŠŸèƒ½")
    print("è¯·ç¡®ä¿å·²ç»åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€äº†æ”¿ç­–æ–‡ä»¶åº“é¡µé¢")
    print("=" * 60)
    
    test_crawler = PlaywrightTestCrawler()
    return test_crawler

if __name__ == "__main__":
    test_instance = main() 